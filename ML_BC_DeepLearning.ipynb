{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1396d80f-c85c-4b76-b947-22929890c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "      <th>Gender_M</th>\n",
       "      <th>Marital_Status_Married</th>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <th>Card_Category_Gold</th>\n",
       "      <th>Card_Category_Platinum</th>\n",
       "      <th>Card_Category_Silver</th>\n",
       "      <th>Attrition_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.094617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.192663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240310</td>\n",
       "      <td>0.294035</td>\n",
       "      <td>0.051051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.667461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.186782</td>\n",
       "      <td>0.362362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.071973</td>\n",
       "      <td>0.747716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449612</td>\n",
       "      <td>0.248950</td>\n",
       "      <td>0.493493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.220049</td>\n",
       "      <td>0.285658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379845</td>\n",
       "      <td>0.166620</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Dependent_count  Education_Level  Income_Category  \\\n",
       "0      0.489362              1.0                4                1   \n",
       "1      0.531915              0.8                2                0   \n",
       "2      0.255319              0.6                2                2   \n",
       "3      0.680851              0.2                2                4   \n",
       "4      0.191489              0.8                2                2   \n",
       "\n",
       "   Months_on_book  Total_Relationship_Count  Months_Inactive_12_mon  \\\n",
       "0        0.604651                       0.4                0.166667   \n",
       "1        0.674419                       0.4                0.333333   \n",
       "2        0.348837                       0.4                0.166667   \n",
       "3        0.813953                       0.6                0.500000   \n",
       "4        0.209302                       0.4                0.500000   \n",
       "\n",
       "   Contacts_Count_12_mon  Credit_Limit  Total_Revolving_Bal  ...  \\\n",
       "0               0.666667      0.094617             0.000000  ...   \n",
       "1               0.500000      1.000000             0.700437  ...   \n",
       "2               0.333333      0.096975             0.667461  ...   \n",
       "3               0.166667      0.071973             0.747716  ...   \n",
       "4               0.500000      0.220049             0.285658  ...   \n",
       "\n",
       "   Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  Avg_Utilization_Ratio  Gender_M  \\\n",
       "0        0.131783             0.192663               0.000000       0.0   \n",
       "1        0.240310             0.294035               0.051051       1.0   \n",
       "2        0.193798             0.186782               0.362362       1.0   \n",
       "3        0.449612             0.248950               0.493493       0.0   \n",
       "4        0.379845             0.166620               0.082082       1.0   \n",
       "\n",
       "   Marital_Status_Married  Marital_Status_Single  Card_Category_Gold  \\\n",
       "0                     1.0                    0.0                 0.0   \n",
       "1                     0.0                    1.0                 0.0   \n",
       "2                     1.0                    0.0                 0.0   \n",
       "3                     0.0                    1.0                 0.0   \n",
       "4                     0.0                    1.0                 0.0   \n",
       "\n",
       "   Card_Category_Platinum  Card_Category_Silver  Attrition_Flag  \n",
       "0                     0.0                   0.0               1  \n",
       "1                     0.0                   0.0               1  \n",
       "2                     0.0                   0.0               1  \n",
       "3                     0.0                   0.0               1  \n",
       "4                     0.0                   0.0               1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_data_path = 'balanced_train_data.csv'\n",
    "validation_data_path = 'validation_data.csv'\n",
    "test_data_path = 'test_data.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "validation_data = pd.read_csv(validation_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9df3726-f97b-4d76-838f-e515f75b6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.5002, Validation Loss: 0.4029, Accuracy: 0.8286, Precision: 0.9334, Recall: 0.8571, F1 Score: 0.8936, ROC AUC: 0.8496\n",
      "Epoch 2/10, Training Loss: 0.3746, Validation Loss: 0.3404, Accuracy: 0.8528, Precision: 0.9505, Recall: 0.8700, F1 Score: 0.9085, ROC AUC: 0.9016\n",
      "Epoch 3/10, Training Loss: 0.3337, Validation Loss: 0.3341, Accuracy: 0.8523, Precision: 0.9534, Recall: 0.8665, F1 Score: 0.9079, ROC AUC: 0.9170\n",
      "Epoch 4/10, Training Loss: 0.3078, Validation Loss: 0.3621, Accuracy: 0.8459, Precision: 0.9621, Recall: 0.8500, F1 Score: 0.9026, ROC AUC: 0.9197\n",
      "Epoch 5/10, Training Loss: 0.2911, Validation Loss: 0.3215, Accuracy: 0.8662, Precision: 0.9685, Recall: 0.8688, F1 Score: 0.9160, ROC AUC: 0.9329\n",
      "Epoch 6/10, Training Loss: 0.2880, Validation Loss: 0.2851, Accuracy: 0.8825, Precision: 0.9632, Recall: 0.8941, F1 Score: 0.9274, ROC AUC: 0.9341\n",
      "Epoch 7/10, Training Loss: 0.2710, Validation Loss: 0.2653, Accuracy: 0.8933, Precision: 0.9667, Recall: 0.9041, F1 Score: 0.9343, ROC AUC: 0.9364\n",
      "Epoch 8/10, Training Loss: 0.2655, Validation Loss: 0.2748, Accuracy: 0.8869, Precision: 0.9712, Recall: 0.8918, F1 Score: 0.9298, ROC AUC: 0.9463\n",
      "Epoch 9/10, Training Loss: 0.2548, Validation Loss: 0.2819, Accuracy: 0.8889, Precision: 0.9718, Recall: 0.8935, F1 Score: 0.9310, ROC AUC: 0.9394\n",
      "Epoch 10/10, Training Loss: 0.2489, Validation Loss: 0.2344, Accuracy: 0.9037, Precision: 0.9677, Recall: 0.9159, F1 Score: 0.9411, ROC AUC: 0.9502\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.9141, Precision: 0.9716, Recall: 0.9247, F1 Score: 0.9476, ROC AUC: 0.9544\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Splitting data into features and target\n",
    "X_train = train_data.drop(columns=['Attrition_Flag'])\n",
    "y_train = train_data['Attrition_Flag']\n",
    "\n",
    "X_val = validation_data.drop(columns=['Attrition_Flag'])\n",
    "y_val = validation_data['Attrition_Flag']\n",
    "\n",
    "X_test = test_data.drop(columns=['Attrition_Flag'])\n",
    "y_test = test_data['Attrition_Flag']\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for training, validation, and test datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Fully Connected Neural Network\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = FCNN(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_outputs.extend(outputs.numpy().flatten())\n",
    "            val_targets.extend(y_batch.numpy().flatten())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = [1 if p > 0.5 else 0 for p in val_outputs]\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    val_precision = precision_score(val_targets, val_predictions)\n",
    "    val_recall = recall_score(val_targets, val_predictions)\n",
    "    val_f1 = f1_score(val_targets, val_predictions)\n",
    "    val_roc_auc = roc_auc_score(val_targets, val_outputs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "          f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}, \"\n",
    "          f\"ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# Test phase\n",
    "model.eval()\n",
    "test_outputs = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        test_outputs.extend(outputs.numpy().flatten())\n",
    "        test_targets.extend(y_batch.numpy().flatten())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_predictions = [1 if p > 0.5 else 0 for p in test_outputs]\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "test_precision = precision_score(test_targets, test_predictions)\n",
    "test_recall = recall_score(test_targets, test_predictions)\n",
    "test_f1 = f1_score(test_targets, test_predictions)\n",
    "test_roc_auc = roc_auc_score(test_targets, test_outputs)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}, ROC AUC: {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd553f11-c15f-4281-9789-eabb2ddeb622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.8983\n",
      "Best Hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'hidden_sizes': (128, 256, 128)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'batch_size': [32, 64],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'hidden_sizes': [(64, 128, 64), (128, 256, 128)]\n",
    "}\n",
    "\n",
    "# Grid Search over all combinations\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for lr, batch_size, dropout_rate, hidden_sizes in itertools.product(\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['dropout_rate'],\n",
    "    param_grid['hidden_sizes']\n",
    "):\n",
    "    # Define the model with current hyperparameters\n",
    "    class FCNN(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(FCNN, self).__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_sizes[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_sizes[1]),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_sizes[2]),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_sizes[2], 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = FCNN(X_train.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop (simplified for testing parameters)\n",
    "    for epoch in range(5):  # Train only a few epochs per grid search iteration\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    model.eval()\n",
    "    val_outputs = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            val_outputs.extend(outputs.numpy().flatten())\n",
    "            val_targets.extend(y_batch.numpy().flatten())\n",
    "    val_predictions = [1 if p > 0.5 else 0 for p in val_outputs]\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "    # Track best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_params = {\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'hidden_sizes': hidden_sizes\n",
    "        }\n",
    "\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7579c8-b5d5-49d3-a0fb-f6f1482efcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.4094, Validation Loss: 0.3334, Accuracy: 0.8548, Precision: 0.9466, Recall: 0.8765, F1 Score: 0.9102, ROC AUC: 0.8973\n",
      "Epoch 2/30, Training Loss: 0.3032, Validation Loss: 0.3075, Accuracy: 0.8736, Precision: 0.9570, Recall: 0.8894, F1 Score: 0.9220, ROC AUC: 0.9212\n",
      "Epoch 3/30, Training Loss: 0.2686, Validation Loss: 0.3339, Accuracy: 0.8548, Precision: 0.9631, Recall: 0.8600, F1 Score: 0.9086, ROC AUC: 0.9229\n",
      "Epoch 4/30, Training Loss: 0.2449, Validation Loss: 0.3004, Accuracy: 0.8726, Precision: 0.9634, Recall: 0.8818, F1 Score: 0.9208, ROC AUC: 0.9354\n",
      "Epoch 5/30, Training Loss: 0.2263, Validation Loss: 0.2947, Accuracy: 0.8795, Precision: 0.9691, Recall: 0.8847, F1 Score: 0.9250, ROC AUC: 0.9377\n",
      "Epoch 6/30, Training Loss: 0.2115, Validation Loss: 0.2661, Accuracy: 0.8889, Precision: 0.9659, Recall: 0.8994, F1 Score: 0.9315, ROC AUC: 0.9414\n",
      "Epoch 7/30, Training Loss: 0.2064, Validation Loss: 0.2510, Accuracy: 0.8973, Precision: 0.9680, Recall: 0.9076, F1 Score: 0.9369, ROC AUC: 0.9469\n",
      "Epoch 8/30, Training Loss: 0.1886, Validation Loss: 0.2334, Accuracy: 0.9012, Precision: 0.9613, Recall: 0.9194, F1 Score: 0.9399, ROC AUC: 0.9485\n",
      "Epoch 9/30, Training Loss: 0.1874, Validation Loss: 0.2419, Accuracy: 0.9022, Precision: 0.9729, Recall: 0.9088, F1 Score: 0.9398, ROC AUC: 0.9570\n",
      "Epoch 10/30, Training Loss: 0.1784, Validation Loss: 0.2276, Accuracy: 0.9121, Precision: 0.9692, Recall: 0.9247, F1 Score: 0.9464, ROC AUC: 0.9548\n",
      "Epoch 11/30, Training Loss: 0.1774, Validation Loss: 0.2100, Accuracy: 0.9151, Precision: 0.9636, Recall: 0.9341, F1 Score: 0.9486, ROC AUC: 0.9572\n",
      "Epoch 12/30, Training Loss: 0.1703, Validation Loss: 0.2045, Accuracy: 0.9136, Precision: 0.9658, Recall: 0.9300, F1 Score: 0.9476, ROC AUC: 0.9597\n",
      "Epoch 13/30, Training Loss: 0.1616, Validation Loss: 0.2190, Accuracy: 0.9160, Precision: 0.9676, Recall: 0.9312, F1 Score: 0.9490, ROC AUC: 0.9563\n",
      "Epoch 14/30, Training Loss: 0.1561, Validation Loss: 0.1957, Accuracy: 0.9190, Precision: 0.9706, Recall: 0.9318, F1 Score: 0.9508, ROC AUC: 0.9622\n",
      "Epoch 15/30, Training Loss: 0.1532, Validation Loss: 0.2193, Accuracy: 0.9086, Precision: 0.9702, Recall: 0.9194, F1 Score: 0.9441, ROC AUC: 0.9608\n",
      "Epoch 16/30, Training Loss: 0.1462, Validation Loss: 0.2038, Accuracy: 0.9180, Precision: 0.9654, Recall: 0.9359, F1 Score: 0.9504, ROC AUC: 0.9601\n",
      "Epoch 17/30, Training Loss: 0.1453, Validation Loss: 0.1933, Accuracy: 0.9264, Precision: 0.9714, Recall: 0.9400, F1 Score: 0.9555, ROC AUC: 0.9623\n",
      "Epoch 18/30, Training Loss: 0.1437, Validation Loss: 0.1969, Accuracy: 0.9240, Precision: 0.9662, Recall: 0.9424, F1 Score: 0.9541, ROC AUC: 0.9617\n",
      "Epoch 19/30, Training Loss: 0.1340, Validation Loss: 0.2129, Accuracy: 0.9185, Precision: 0.9723, Recall: 0.9294, F1 Score: 0.9504, ROC AUC: 0.9609\n",
      "Epoch 20/30, Training Loss: 0.1332, Validation Loss: 0.1972, Accuracy: 0.9269, Precision: 0.9680, Recall: 0.9441, F1 Score: 0.9559, ROC AUC: 0.9624\n",
      "Epoch 21/30, Training Loss: 0.1322, Validation Loss: 0.2326, Accuracy: 0.9081, Precision: 0.9749, Recall: 0.9141, F1 Score: 0.9435, ROC AUC: 0.9635\n",
      "Epoch 22/30, Training Loss: 0.1288, Validation Loss: 0.2060, Accuracy: 0.9210, Precision: 0.9724, Recall: 0.9324, F1 Score: 0.9520, ROC AUC: 0.9645\n",
      "Epoch 23/30, Training Loss: 0.1281, Validation Loss: 0.2126, Accuracy: 0.9151, Precision: 0.9716, Recall: 0.9259, F1 Score: 0.9482, ROC AUC: 0.9639\n",
      "Epoch 24/30, Training Loss: 0.1286, Validation Loss: 0.1954, Accuracy: 0.9319, Precision: 0.9774, Recall: 0.9406, F1 Score: 0.9586, ROC AUC: 0.9661\n",
      "Epoch 25/30, Training Loss: 0.1152, Validation Loss: 0.1966, Accuracy: 0.9230, Precision: 0.9651, Recall: 0.9424, F1 Score: 0.9536, ROC AUC: 0.9640\n",
      "Epoch 26/30, Training Loss: 0.1169, Validation Loss: 0.1996, Accuracy: 0.9225, Precision: 0.9696, Recall: 0.9371, F1 Score: 0.9530, ROC AUC: 0.9638\n",
      "Epoch 27/30, Training Loss: 0.1199, Validation Loss: 0.1918, Accuracy: 0.9294, Precision: 0.9733, Recall: 0.9418, F1 Score: 0.9572, ROC AUC: 0.9665\n",
      "Epoch 28/30, Training Loss: 0.1207, Validation Loss: 0.1848, Accuracy: 0.9314, Precision: 0.9733, Recall: 0.9441, F1 Score: 0.9585, ROC AUC: 0.9683\n",
      "Epoch 29/30, Training Loss: 0.1114, Validation Loss: 0.1853, Accuracy: 0.9348, Precision: 0.9667, Recall: 0.9553, F1 Score: 0.9609, ROC AUC: 0.9661\n",
      "Epoch 30/30, Training Loss: 0.1139, Validation Loss: 0.1915, Accuracy: 0.9279, Precision: 0.9681, Recall: 0.9453, F1 Score: 0.9565, ROC AUC: 0.9643\n",
      "\n",
      "Test Results with Best Hyperparameters:\n",
      "Accuracy: 0.9358, Precision: 0.9706, Recall: 0.9524, F1 Score: 0.9614, ROC AUC: 0.9717\n"
     ]
    }
   ],
   "source": [
    "# Define the best model with selected hyperparameters\n",
    "class FCNNBest(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FCNNBest, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = FCNNBest(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 30\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_outputs.extend(outputs.numpy().flatten())\n",
    "            val_targets.extend(y_batch.numpy().flatten())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = [1 if p > 0.5 else 0 for p in val_outputs]\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    val_precision = precision_score(val_targets, val_predictions)\n",
    "    val_recall = recall_score(val_targets, val_predictions)\n",
    "    val_f1 = f1_score(val_targets, val_predictions)\n",
    "    val_roc_auc = roc_auc_score(val_targets, val_outputs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "          f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}, \"\n",
    "          f\"ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# Final Test Evaluation\n",
    "model.eval()\n",
    "test_outputs = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        test_outputs.extend(outputs.numpy().flatten())\n",
    "        test_targets.extend(y_batch.numpy().flatten())\n",
    "\n",
    "# Test Metrics\n",
    "test_predictions = [1 if p > 0.5 else 0 for p in test_outputs]\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "test_precision = precision_score(test_targets, test_predictions)\n",
    "test_recall = recall_score(test_targets, test_predictions)\n",
    "test_f1 = f1_score(test_targets, test_predictions)\n",
    "test_roc_auc = roc_auc_score(test_targets, test_outputs)\n",
    "\n",
    "print(\"\\nTest Results with Best Hyperparameters:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}, ROC AUC: {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9662924f-e89a-4420-a4bb-c3b9de7e9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.2926, Validation Loss: 0.2916, Accuracy: 0.8854, Precision: 0.9723, Recall: 0.8888, F1 Score: 0.9287, ROC AUC: 0.9441\n",
      "Epoch 2/10, Training Loss: 0.2118, Validation Loss: 0.2830, Accuracy: 0.8844, Precision: 0.9791, Recall: 0.8812, F1 Score: 0.9276, ROC AUC: 0.9547\n",
      "Epoch 3/10, Training Loss: 0.1872, Validation Loss: 0.2596, Accuracy: 0.9101, Precision: 0.9617, Recall: 0.9300, F1 Score: 0.9456, ROC AUC: 0.9492\n",
      "Epoch 4/10, Training Loss: 0.1770, Validation Loss: 0.2232, Accuracy: 0.8993, Precision: 0.9746, Recall: 0.9035, F1 Score: 0.9377, ROC AUC: 0.9590\n",
      "Epoch 5/10, Training Loss: 0.1581, Validation Loss: 0.2255, Accuracy: 0.9096, Precision: 0.9773, Recall: 0.9135, F1 Score: 0.9444, ROC AUC: 0.9601\n",
      "Epoch 6/10, Training Loss: 0.1558, Validation Loss: 0.2354, Accuracy: 0.9062, Precision: 0.9785, Recall: 0.9082, F1 Score: 0.9420, ROC AUC: 0.9604\n",
      "Epoch 7/10, Training Loss: 0.1371, Validation Loss: 0.2197, Accuracy: 0.9106, Precision: 0.9780, Recall: 0.9141, F1 Score: 0.9450, ROC AUC: 0.9625\n",
      "Epoch 8/10, Training Loss: 0.1284, Validation Loss: 0.2555, Accuracy: 0.9185, Precision: 0.9555, Recall: 0.9471, F1 Score: 0.9513, ROC AUC: 0.9475\n",
      "Epoch 9/10, Training Loss: 0.1213, Validation Loss: 0.2274, Accuracy: 0.9259, Precision: 0.9680, Recall: 0.9429, F1 Score: 0.9553, ROC AUC: 0.9606\n",
      "Epoch 10/10, Training Loss: 0.1129, Validation Loss: 0.2678, Accuracy: 0.9091, Precision: 0.9797, Recall: 0.9106, F1 Score: 0.9439, ROC AUC: 0.9675\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.9195, Precision: 0.9788, Recall: 0.9241, F1 Score: 0.9507, ROC AUC: 0.9685\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Identify numerical columns (categorical columns are already encoded)\n",
    "numerical_cols = [col for col in X_train.columns if col not in ['Attrition_Flag']]\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# 1. Create PyTorch Dataset\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "test_dataset = TabularDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 2. Define TabTransformer Model\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim, num_heads, num_layers):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=attention_dim, nhead=num_heads, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_in = nn.Linear(input_dim, attention_dim)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(attention_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input to attention space\n",
    "        x = self.fc_in(x).unsqueeze(1)  # Add sequence dimension\n",
    "        # Apply attention layers\n",
    "        for layer in self.attention_layers:\n",
    "            x = layer(x)\n",
    "        # Global average pooling and output\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.fc_out(pooled)\n",
    "\n",
    "# Model Configuration\n",
    "input_dim = X_train.shape[1]\n",
    "attention_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "model = TabTransformer(input_dim, attention_dim, num_heads, num_layers)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_outputs.extend(outputs.numpy().flatten())\n",
    "            val_targets.extend(y_batch.numpy().flatten())\n",
    "\n",
    "    val_predictions = [1 if p > 0.5 else 0 for p in val_outputs]\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    val_precision = precision_score(val_targets, val_predictions)\n",
    "    val_recall = recall_score(val_targets, val_predictions)\n",
    "    val_f1 = f1_score(val_targets, val_predictions)\n",
    "    val_roc_auc = roc_auc_score(val_targets, val_outputs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "          f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}, \"\n",
    "          f\"ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# 4. Test Evaluation\n",
    "model.eval()\n",
    "test_outputs = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        test_outputs.extend(outputs.numpy().flatten())\n",
    "        test_targets.extend(y_batch.numpy().flatten())\n",
    "\n",
    "# Test Metrics\n",
    "test_predictions = [1 if p > 0.5 else 0 for p in test_outputs]\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "test_precision = precision_score(test_targets, test_predictions)\n",
    "test_recall = recall_score(test_targets, test_predictions)\n",
    "test_f1 = f1_score(test_targets, test_predictions)\n",
    "test_roc_auc = roc_auc_score(test_targets, test_outputs)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}, ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
